{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e42acc3c",
      "metadata": {
        "id": "e42acc3c"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msfasha/307401-Big-Data/blob/main/20251/Module_6_introduction_to_databricks/1_NYC_Taxi_Analytics_Statistics_and_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f573078",
      "metadata": {
        "id": "4f573078"
      },
      "source": [
        "# Analytics on the NYC Taxi Dataset Using Parquet & Spark\n",
        "\n",
        "This notebook provides ready-to-use analytical, statistical, and machine-learning workflows on the NYC Taxi dataset stored in Parquet format using Apache Spark."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7b3b513",
      "metadata": {
        "id": "a7b3b513"
      },
      "source": [
        "## 1. Dataset Motivation\n",
        "\n",
        "The NYC Taxi dataset combines time-based, numerical, and categorical data at scale, making it suitable for statistical analysis and machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d1abbf2",
      "metadata": {
        "id": "9d1abbf2"
      },
      "source": [
        "## 2. Load Data from Parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e6c80d",
      "metadata": {},
      "source": [
        "**What this does**: starts a Spark session, downloads the October 2025 NYC yellow taxi parquet (via wget), and loads it locally if missing.\n",
        "**Environment note**: if wget is unavailable, download the file manually to the working directory.\n",
        "**Re-run guidance**: update the URL when a newer monthly parquet is needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3187f5d7",
      "metadata": {
        "id": "3187f5d7"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder.appName(\"NYTaxiAnalytics\").getOrCreate()\n",
        "\n",
        "# Define the URL and local path for the dataset\n",
        "dataset_url = \"https://github.com/msfasha/307401-Big-Data/raw/main/datasets/yellow_tripdata_2025-10.parquet\"\n",
        "local_parquet_path = \"yellow_tripdata_2025-10.parquet\"\n",
        "\n",
        "# Download the file if it doesn't exist locally\n",
        "if not os.path.exists(local_parquet_path):\n",
        "    print(f\"Downloading {dataset_url} to {local_parquet_path}...\")\n",
        "    # Using subprocess.run for better control and error handling than !wget\n",
        "    import subprocess\n",
        "    try:\n",
        "        subprocess.run([\"wget\", dataset_url], check=True)\n",
        "        print(\"Download complete.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error downloading file: {e}\")\n",
        "        # Handle the error, maybe raise it again or exit\n",
        "        raise\n",
        "\n",
        "# Read the Parquet file from the local path\n",
        "taxi_df = spark.read.parquet(local_parquet_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "335fdb3b",
      "metadata": {
        "id": "335fdb3b"
      },
      "source": [
        "## 3. Schema Inspection and Profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b775a0b",
      "metadata": {},
      "source": [
        "**Purpose**: sanity-check the loaded parquet by printing schema and forcing a full scan with count().\n",
        "**Caution**: count() can be slow on large data; cancel if it runs too long in your environment.\n",
        "**Outcome**: confirm columns and data types match expectations before proceeding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d460be16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d460be16",
        "outputId": "a19cf0b9-2e79-4a44-be09-168dd2e4b74d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- VendorID: integer (nullable = true)\n",
            " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
            " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
            " |-- passenger_count: long (nullable = true)\n",
            " |-- trip_distance: double (nullable = true)\n",
            " |-- RatecodeID: long (nullable = true)\n",
            " |-- store_and_fwd_flag: string (nullable = true)\n",
            " |-- PULocationID: integer (nullable = true)\n",
            " |-- DOLocationID: integer (nullable = true)\n",
            " |-- payment_type: long (nullable = true)\n",
            " |-- fare_amount: double (nullable = true)\n",
            " |-- extra: double (nullable = true)\n",
            " |-- mta_tax: double (nullable = true)\n",
            " |-- tip_amount: double (nullable = true)\n",
            " |-- tolls_amount: double (nullable = true)\n",
            " |-- improvement_surcharge: double (nullable = true)\n",
            " |-- total_amount: double (nullable = true)\n",
            " |-- congestion_surcharge: double (nullable = true)\n",
            " |-- Airport_fee: double (nullable = true)\n",
            " |-- cbd_congestion_fee: double (nullable = true)\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4428699"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "taxi_df.printSchema()\n",
        "taxi_df.count()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0791c889",
      "metadata": {
        "id": "0791c889"
      },
      "source": [
        "## 4. Summary Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16724447",
      "metadata": {},
      "source": [
        "**Purpose**: compute quick descriptive stats for distance, fare, and passenger counts.\n",
        "**Why**: spot obvious outliers/nulls before modeling; the column list stays narrow for readability.\n",
        "**Next**: widen the column list if you need more features summarized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a551f1ad",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a551f1ad",
        "outputId": "750a558e-15d2-4ca7-edec-3b03b2a8557d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+------------------+------------------+------------------+\n",
            "|summary|     trip_distance|       fare_amount|   passenger_count|\n",
            "+-------+------------------+------------------+------------------+\n",
            "|  count|           4428699|           4428699|           3437812|\n",
            "|   mean|6.6967439196033824|18.255545755085404|1.2748128751659487|\n",
            "| stddev| 656.9619547390072|19.688461694744408|0.6913772296675526|\n",
            "|    min|               0.0|            -800.0|                 0|\n",
            "|    max|         276333.48|            1071.9|                 9|\n",
            "+-------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "taxi_df.select(\n",
        "    \"trip_distance\",\n",
        "    \"fare_amount\",\n",
        "    \"passenger_count\"\n",
        ").describe().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bdaa300",
      "metadata": {
        "id": "2bdaa300"
      },
      "source": [
        "## 5. Distribution Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b45ce96d",
      "metadata": {},
      "source": [
        "**Goal**: bin fares into $5 ranges to view distribution shape.\n",
        "**Data quality note**: negative or extreme fares stay visible in their own buckets.\n",
        "**Adjust**: change the divisor to alter bin width if the histogram is too coarse/fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dacf23c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dacf23c",
        "outputId": "fc1580af-6ea2-4df9-9830-e73cff815ce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----+\n",
            "|fare_bucket|count|\n",
            "+-----------+-----+\n",
            "|       -800|    3|\n",
            "|       -795|    1|\n",
            "|       -750|    1|\n",
            "|       -700|    2|\n",
            "|       -665|    1|\n",
            "|       -650|    1|\n",
            "|       -645|    1|\n",
            "|       -640|    1|\n",
            "|       -620|    1|\n",
            "|       -600|    3|\n",
            "|       -550|    1|\n",
            "|       -520|    1|\n",
            "|       -500|   14|\n",
            "|       -495|    2|\n",
            "|       -490|    1|\n",
            "|       -480|    4|\n",
            "|       -475|    1|\n",
            "|       -455|    1|\n",
            "|       -450|    2|\n",
            "|       -420|    2|\n",
            "+-----------+-----+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import floor\n",
        "\n",
        "taxi_df.withColumn(\n",
        "    \"fare_bucket\", floor(col(\"fare_amount\") / 5) * 5\n",
        ").groupBy(\"fare_bucket\").count().orderBy(\"fare_bucket\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b20d6190",
      "metadata": {
        "id": "b20d6190"
      },
      "source": [
        "## 6. Time-Based Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a6854f2",
      "metadata": {},
      "source": [
        "**Goal**: count trips by pickup hour to see demand peaks.\n",
        "**Assumption**: timestamps already in desired timezone; adjust upstream if needed.\n",
        "**Use**: results drive staffing or surge-pricing style decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "726465d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "726465d0",
        "outputId": "5f65a7a1-6c29-4dbc-c0d0-e1e3aef29c6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+------+\n",
            "|pickup_hour| count|\n",
            "+-----------+------+\n",
            "|          0|131173|\n",
            "|          1| 85061|\n",
            "|          2| 56379|\n",
            "|          3| 37668|\n",
            "|          4| 30050|\n",
            "|          5| 34947|\n",
            "|          6| 72168|\n",
            "|          7|135906|\n",
            "|          8|179024|\n",
            "|          9|189819|\n",
            "|         10|194409|\n",
            "|         11|205732|\n",
            "|         12|218286|\n",
            "|         13|227499|\n",
            "|         14|244760|\n",
            "|         15|253651|\n",
            "|         16|261671|\n",
            "|         17|295831|\n",
            "|         18|315195|\n",
            "|         19|287385|\n",
            "+-----------+------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import hour\n",
        "\n",
        "taxi_df.withColumn(\n",
        "    \"pickup_hour\", hour(\"tpep_pickup_datetime\")\n",
        ").groupBy(\"pickup_hour\").count().orderBy(\"pickup_hour\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28e0bb16",
      "metadata": {
        "id": "28e0bb16"
      },
      "source": [
        "## 7. Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e996c45",
      "metadata": {},
      "source": [
        "**Goal**: derive trip duration (minutes) and filter out implausible records.\n",
        "**Filters**: duration 1–180 min, positive fare/distance, and non-null passenger counts.\n",
        "**Tip**: review clean_df head to confirm filters didn’t drop too much data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66c6543d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66c6543d",
        "outputId": "27bca5be-e274-4181-ab39-94cf0209bfa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+------------------+\n",
            "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee| trip_duration_min|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+------------------+\n",
            "|       1| 2025-10-01 00:15:32|  2025-10-01 01:04:03|              1|         17.2|         2|                 N|         132|         107|           1|       70.0|  5.0|    0.5|       0.0|        6.94|                  1.0|       83.44|                 2.5|       1.75|              0.75|48.516666666666666|\n",
            "|       2| 2025-10-01 00:08:54|  2025-10-01 00:14:44|              1|         2.75|         1|                 N|         263|         229|           1|       12.8|  1.0|    0.5|      3.71|         0.0|                  1.0|       22.26|                 2.5|        0.0|              0.75| 5.833333333333333|\n",
            "|       1| 2025-10-01 00:58:48|  2025-10-01 01:04:40|              1|          1.3|         1|                 N|         211|         231|           2|        7.9| 4.25|    0.5|       0.0|         0.0|                  1.0|       13.65|                 2.5|        0.0|              0.75| 5.866666666666666|\n",
            "|       2| 2025-10-01 00:39:51|  2025-10-01 00:49:40|              1|         2.88|         1|                 N|         230|         151|           1|       14.2|  1.0|    0.5|      3.99|         0.0|                  1.0|       23.94|                 2.5|        0.0|              0.75| 9.816666666666666|\n",
            "|       1| 2025-10-01 00:30:54|  2025-10-01 00:37:50|              1|          1.6|         1|                 N|         237|         142|           2|        9.3|  3.5|    0.5|       0.0|         0.0|                  1.0|        14.3|                 2.5|        0.0|               0.0| 6.933333333333334|\n",
            "|       2| 2025-10-01 00:10:12|  2025-10-01 00:17:29|              1|         2.81|         1|                 N|         142|         166|           1|       12.8|  1.0|    0.5|      3.56|         0.0|                  1.0|       21.36|                 2.5|        0.0|               0.0| 7.283333333333333|\n",
            "|       2| 2025-10-01 00:48:19|  2025-10-01 01:01:02|              1|         2.17|         1|                 N|         230|         246|           1|       13.5|  1.0|    0.5|       0.0|         0.0|                  1.0|       19.25|                 2.5|        0.0|              0.75|12.716666666666667|\n",
            "|       2| 2025-10-01 00:08:44|  2025-10-01 00:24:17|              1|         3.71|         1|                 N|         140|           7|           1|       19.1|  1.0|    0.5|      6.21|         0.0|                  1.0|       31.06|                 2.5|        0.0|              0.75|             15.55|\n",
            "|       2| 2025-10-01 00:25:23|  2025-10-01 00:33:02|              1|          1.2|         1|                 N|         234|         249|           2|        8.6|  1.0|    0.5|       0.0|         0.0|                  1.0|       14.35|                 2.5|        0.0|              0.75|              7.65|\n",
            "|       1| 2025-10-01 00:25:43|  2025-10-01 00:42:51|              1|          3.1|         1|                 N|         231|         230|           2|       18.4| 4.25|    0.5|       0.0|         0.0|                  1.0|       24.15|                 2.5|        0.0|              0.75|17.133333333333333|\n",
            "|       2| 2025-10-01 00:50:07|  2025-10-01 00:56:14|              2|         0.69|         1|                 N|          33|          65|           2|        7.2|  1.0|    0.5|       0.0|         0.0|                  1.0|         9.7|                 0.0|        0.0|               0.0| 6.116666666666666|\n",
            "|       2| 2025-10-01 00:08:03|  2025-10-01 00:18:53|              1|          1.1|         1|                 N|         100|         229|           2|       10.0|  1.0|    0.5|       0.0|         0.0|                  1.0|       15.75|                 2.5|        0.0|              0.75|10.833333333333334|\n",
            "|       2| 2025-10-01 00:44:12|  2025-10-01 00:57:02|              1|         1.41|         1|                 N|         164|         246|           1|       12.8|  1.0|    0.5|      3.71|         0.0|                  1.0|       22.26|                 2.5|        0.0|              0.75|12.833333333333334|\n",
            "|       1| 2025-10-01 00:58:59|  2025-10-01 01:02:25|              1|          1.8|         1|                 N|         137|         232|           1|        9.3| 4.25|    0.5|      3.75|         0.0|                  1.0|        18.8|                 2.5|        0.0|              0.75| 3.433333333333333|\n",
            "|       2| 2025-10-01 00:37:37|  2025-10-01 00:51:58|              1|         3.06|         1|                 N|         140|         238|           1|       17.0|  1.0|    0.5|       4.4|         0.0|                  1.0|        26.4|                 2.5|        0.0|               0.0|             14.35|\n",
            "|       2| 2025-10-01 00:00:59|  2025-10-01 00:09:31|              1|         1.63|         1|                 N|         164|          79|           1|       10.7|  1.0|    0.5|      3.29|         0.0|                  1.0|       19.74|                 2.5|        0.0|              0.75| 8.533333333333333|\n",
            "|       2| 2025-10-01 00:11:47|  2025-10-01 00:21:15|              1|         1.79|         1|                 N|          43|         186|           1|       10.7|  1.0|    0.5|      3.29|         0.0|                  1.0|       19.74|                 2.5|        0.0|              0.75| 9.466666666666667|\n",
            "|       2| 2025-10-01 00:26:14|  2025-10-01 00:35:18|              1|         1.68|         1|                 N|          90|         211|           1|       10.7|  1.0|    0.5|      3.29|         0.0|                  1.0|       19.74|                 2.5|        0.0|              0.75| 9.066666666666666|\n",
            "|       2| 2025-10-01 00:38:48|  2025-10-01 00:47:57|              1|         1.82|         1|                 N|         114|         100|           1|       11.4|  1.0|    0.5|       3.0|         0.0|                  1.0|       20.15|                 2.5|        0.0|              0.75|              9.15|\n",
            "|       1| 2025-10-01 00:05:04|  2025-10-01 00:09:26|              1|          0.7|         1|                 N|         163|         162|           1|        5.8| 4.25|    0.5|       2.3|         0.0|                  1.0|       13.85|                 2.5|        0.0|              0.75| 4.366666666666666|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+------------------+\n",
            "only showing top 20 rows\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import unix_timestamp\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "taxi_df = taxi_df.withColumn(\n",
        "    \"trip_duration_min\",\n",
        "    (unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp(\"tpep_pickup_datetime\")) / 60\n",
        ")\n",
        "\n",
        "clean_df = taxi_df.filter(\n",
        "    (col(\"trip_duration_min\").isNotNull()) &\n",
        "    (col(\"trip_duration_min\") > 1) &\n",
        "    (col(\"trip_duration_min\") < 180) &\n",
        "    (col(\"fare_amount\").isNotNull()) &\n",
        "    (col(\"fare_amount\") > 0) &\n",
        "    (col(\"trip_distance\").isNotNull()) &\n",
        "    (col(\"trip_distance\") > 0) &\n",
        "    (col(\"passenger_count\").isNotNull()) # Added filter for null passenger_count\n",
        ")\n",
        "\n",
        "clean_df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d1a9275",
      "metadata": {
        "id": "2d1a9275"
      },
      "source": [
        "## 8. Regression: Fare Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb88654b",
      "metadata": {},
      "source": [
        "**Goal**: assemble feature vectors (distance, duration, passengers) and split train/test for fare regression.\n",
        "**Reproducibility**: fixed seed keeps splits stable across runs.\n",
        "**Extension**: add more columns to inputCols if you engineer extra signals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3370def",
      "metadata": {
        "id": "a3370def"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"trip_distance\", \"trip_duration_min\", \"passenger_count\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "ml_df = assembler.transform(clean_df).select(\"features\", \"fare_amount\")\n",
        "train_df, test_df = ml_df.randomSplit([0.8, 0.2], seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48917942",
      "metadata": {},
      "source": [
        "**Goal**: train linear regression on the engineered features to predict fare_amount.\n",
        "**Tuning**: leave regularization defaults for now; adjust if you observe overfit.\n",
        "**Note**: predictions are reused in the next evaluation cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf4cf90",
      "metadata": {
        "id": "0cf4cf90"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "lr = LinearRegression(labelCol=\"fare_amount\")\n",
        "lr_model = lr.fit(train_df)\n",
        "predictions = lr_model.transform(test_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c25b743b",
      "metadata": {},
      "source": [
        "**Goal**: evaluate fare regression with RMSE on the held-out test set.\n",
        "**Interpretation**: lower RMSE is better; compare against naive baselines (e.g., mean fare).\n",
        "**Optional**: switch metricName to mae if absolute errors matter more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dca2e1b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dca2e1b3",
        "outputId": "9d2ffba7-2ce1-4368-a057-b0690fa2faff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7.497843749737397"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "RegressionEvaluator(\n",
        "    labelCol=\"fare_amount\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"rmse\"\n",
        ").evaluate(predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da08b561",
      "metadata": {
        "id": "da08b561"
      },
      "source": [
        "## 9. Classification: High Fare Trips"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cda6ced0",
      "metadata": {},
      "source": [
        "**Goal**: label trips as high_fare (> $50) and build features for classification.\n",
        "**Rationale**: reuse same feature vector as regression for consistency.\n",
        "**Tweak**: adjust the $50 threshold to your business rule for “high fare.”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70a33c25",
      "metadata": {
        "id": "70a33c25"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "labeled_df = clean_df.withColumn(\n",
        "    \"high_fare\", when(col(\"fare_amount\") > 50, 1).otherwise(0)\n",
        ")\n",
        "\n",
        "final_df = assembler.transform(labeled_df).select(\"features\", \"high_fare\")\n",
        "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ea7762",
      "metadata": {},
      "source": [
        "**Goal**: fit logistic regression to classify high_fare vs not.\n",
        "**Regularization**: defaults to L2; tune regParam/elasticNetParam if needed.\n",
        "**Outputs**: predictions include probability for ROC/PR evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1be7c70e",
      "metadata": {
        "id": "1be7c70e"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "log_reg = LogisticRegression(labelCol=\"high_fare\")\n",
        "model = log_reg.fit(train_df)\n",
        "predictions = model.transform(test_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37f1965b",
      "metadata": {},
      "source": [
        "**Goal**: assess classifier quality with ROC AUC (areaUnderROC).\n",
        "**Range**: 0.5 = random, 1.0 = perfect; higher is better.\n",
        "**Option**: set metricName to areaUnderPR if the positive class is rare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53e9a5cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53e9a5cd",
        "outputId": "d1eaaf96-1cdf-48bc-a726-0d7509aae87a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9855387915472841"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "BinaryClassificationEvaluator(\n",
        "    labelCol=\"high_fare\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ").evaluate(predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd95358e",
      "metadata": {
        "id": "dd95358e"
      },
      "source": [
        "## 10. Clustering Trips"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6423de4",
      "metadata": {},
      "source": [
        "**Goal**: cluster trips using the same numeric features (distance, duration, passengers).\n",
        "**Default**: k=5 as a starting guess; refine with silhouette/elbow methods.\n",
        "**Result**: clusterCenters() shows centroids; use transform() later to tag trips with cluster IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5806cb37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5806cb37",
        "outputId": "890567c4-f66c-4ca1-8c80-84cfe51f62c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([1.22969392, 8.36819633, 1.25657773]),\n",
              " array([17.05177296, 89.3303419 ,  1.29901011]),\n",
              " array([ 3.11243249, 19.39885216,  1.27419064]),\n",
              " array([ 7.81761231, 34.23739602,  1.29047851]),\n",
              " array([14.57150232, 56.12243539,  1.3372902 ])]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "cluster_df = assembler.transform(clean_df).select(\"features\")\n",
        "\n",
        "kmeans = KMeans(k=5, seed=42)\n",
        "model = kmeans.fit(cluster_df)\n",
        "\n",
        "model.clusterCenters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a8f01da",
      "metadata": {
        "id": "8a8f01da"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated scalable statistical analysis and machine-learning workflows using Spark and Parquet on the NYC Taxi dataset."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
