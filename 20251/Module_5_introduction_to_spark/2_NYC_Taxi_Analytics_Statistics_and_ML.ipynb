{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e42acc3c",
   "metadata": {},
   "source": [
    "# Analytics on the NYC Taxi Dataset Using Parquet & Spark\n",
    "\n",
    "This notebook provides ready-to-use analytical, statistical, and machine-learning workflows on the NYC Taxi dataset stored in Parquet format using Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b3b513",
   "metadata": {},
   "source": [
    "## 1. Dataset Motivation\n",
    "\n",
    "The NYC Taxi dataset combines time-based, numerical, and categorical data at scale, making it suitable for statistical analysis and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1abbf2",
   "metadata": {},
   "source": [
    "## 2. Load Data from Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NYTaxiAnalytics\").getOrCreate()\n",
    "taxi_df = spark.read.parquet(\"/data/ny_taxi_parquet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335fdb3b",
   "metadata": {},
   "source": [
    "## 3. Schema Inspection and Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d460be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.printSchema()\n",
    "taxi_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0791c889",
   "metadata": {},
   "source": [
    "## 4. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a551f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.select(\n",
    "    \"trip_distance\",\n",
    "    \"fare_amount\",\n",
    "    \"passenger_count\"\n",
    ").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdaa300",
   "metadata": {},
   "source": [
    "## 5. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dacf23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import floor\n",
    "\n",
    "taxi_df.withColumn(\n",
    "    \"fare_bucket\", floor(col(\"fare_amount\") / 5) * 5\n",
    ").groupBy(\"fare_bucket\").count().orderBy(\"fare_bucket\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d6190",
   "metadata": {},
   "source": [
    "## 6. Time-Based Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726465d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "\n",
    "taxi_df.withColumn(\n",
    "    \"pickup_hour\", hour(\"pickup_datetime\")\n",
    ").groupBy(\"pickup_hour\").count().orderBy(\"pickup_hour\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e0bb16",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c6543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "taxi_df = taxi_df.withColumn(\n",
    "    \"trip_duration_min\",\n",
    "    (unix_timestamp(\"dropoff_datetime\") - unix_timestamp(\"pickup_datetime\")) / 60\n",
    ")\n",
    "\n",
    "clean_df = taxi_df.filter(\n",
    "    (col(\"trip_duration_min\") > 1) &\n",
    "    (col(\"trip_duration_min\") < 180) &\n",
    "    (col(\"fare_amount\") > 0) &\n",
    "    (col(\"trip_distance\") > 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1a9275",
   "metadata": {},
   "source": [
    "## 8. Regression: Fare Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3370def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"trip_distance\", \"trip_duration_min\", \"passenger_count\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "ml_df = assembler.transform(clean_df).select(\"features\", \"fare_amount\")\n",
    "train_df, test_df = ml_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf4cf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(labelCol=\"fare_amount\")\n",
    "lr_model = lr.fit(train_df)\n",
    "predictions = lr_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca2e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "RegressionEvaluator(\n",
    "    labelCol=\"fare_amount\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da08b561",
   "metadata": {},
   "source": [
    "## 9. Classification: High Fare Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a33c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "labeled_df = clean_df.withColumn(\n",
    "    \"high_fare\", when(col(\"fare_amount\") > 50, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "final_df = assembler.transform(labeled_df).select(\"features\", \"high_fare\")\n",
    "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be7c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(labelCol=\"high_fare\")\n",
    "model = log_reg.fit(train_df)\n",
    "predictions = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "BinaryClassificationEvaluator(\n",
    "    labelCol=\"high_fare\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd95358e",
   "metadata": {},
   "source": [
    "## 10. Clustering Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5806cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "cluster_df = assembler.transform(clean_df).select(\"features\")\n",
    "\n",
    "kmeans = KMeans(k=5, seed=42)\n",
    "model = kmeans.fit(cluster_df)\n",
    "\n",
    "model.clusterCenters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8f01da",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated scalable statistical analysis and machine-learning workflows using Spark and Parquet on the NYC Taxi dataset."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
