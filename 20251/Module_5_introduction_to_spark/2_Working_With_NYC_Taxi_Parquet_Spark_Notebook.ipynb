{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c336e2",
   "metadata": {},
   "source": [
    "# NYC Taxi Parquet & Advanced Spark Concepts\n",
    "\n",
    "This notebook is designed for teaching Parquet files and advanced Spark concepts using PySpark on Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff3e89",
   "metadata": {},
   "source": [
    "## 1. Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466aff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
    "!tar xf spark-3.5.0-bin-hadoop3.tgz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3e13a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
    "os.environ[\"PATH\"] += \":/content/spark-3.5.0-bin-hadoop3/bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13120ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC Taxi Parquet Teaching\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedec5ab",
   "metadata": {},
   "source": [
    "## 2. Load NYC Taxi Parquet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb05d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: upload parquet files to /content/nyc_taxi_parquet\n",
    "df = spark.read.parquet(\"/content/nyc_taxi_parquet/\")\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f154e",
   "metadata": {},
   "source": [
    "## 3. Column Pruning Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd00ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.select(\"pickup_datetime\", \"fare_amount\") \\\n",
    "  .filter(\"fare_amount > 50\") \\\n",
    "  .explain(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aa08ca",
   "metadata": {},
   "source": [
    "## 4. Spark Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431b7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.rdd.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304eb0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_repart = df.repartition(8)\n",
    "df_repart.rdd.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbff0f9f",
   "metadata": {},
   "source": [
    "## 5. Write Partitioned Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb2d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import year, month\n",
    "\n",
    "df2 = df.withColumn(\"pickup_year\", year(\"pickup_datetime\")) \\\n",
    "        .withColumn(\"pickup_month\", month(\"pickup_datetime\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9319a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2.write \\\n",
    "   .mode(\"overwrite\") \\\n",
    "   .partitionBy(\"pickup_year\", \"pickup_month\") \\\n",
    "   .parquet(\"/content/nyc_taxi_partitioned/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7179c4ec",
   "metadata": {},
   "source": [
    "## 6. Partition Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a9189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_part = spark.read.parquet(\"/content/nyc_taxi_partitioned/\")\n",
    "\n",
    "df_part.filter(\"pickup_year = 2023 AND pickup_month = 1\") \\\n",
    "       .groupBy(\"PULocationID\") \\\n",
    "       .count() \\\n",
    "       .explain(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212693f0",
   "metadata": {},
   "source": [
    "## 7. Small Files Mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e146e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2.coalesce(4) \\\n",
    "   .write \\\n",
    "   .mode(\"overwrite\") \\\n",
    "   .partitionBy(\"pickup_year\", \"pickup_month\") \\\n",
    "   .parquet(\"/content/nyc_taxi_optimized/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a461c0",
   "metadata": {},
   "source": [
    "## 8. Analytical Query Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb0357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df_part.filter(\"pickup_year = 2023\") \\\n",
    "       .groupBy(\"pickup_month\") \\\n",
    "       .agg(\n",
    "           avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "           avg(\"trip_distance\").alias(\"avg_distance\")\n",
    "       ) \\\n",
    "       .orderBy(\"pickup_month\") \\\n",
    "       .show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}