{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4ec7665",
   "metadata": {},
   "source": [
    "## Spark Streaming, Storage Optimizations, and Delta Lake\n",
    "### Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Explain how Structured Streaming works internally\n",
    "- Distinguish event time from processing time\n",
    "- Understand column pruning, predicate pushdown, and partition pruning\n",
    "- Read Spark physical execution plans\n",
    "- Explain what Delta Lake adds on top of Parquet\n",
    "- Use time travel, MERGE, and streaming writes with Delta Lake\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f17fad",
   "metadata": {},
   "source": [
    "## Part 1 – Structured Streaming\n",
    "\n",
    "### Why Streaming?\n",
    "In real systems, data does not arrive all at once. Logs, sensor readings, financial transactions, and user interactions are **continuous and unbounded**.\n",
    "\n",
    "Structured Streaming allows Spark to process this infinite data using the same DataFrame API you already know.\n",
    "\n",
    "Key mental model:\n",
    "\n",
    "**A stream is an unbounded table to which rows are continuously appended.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc44b75f",
   "metadata": {},
   "source": [
    "### Spark Session\n",
    "We begin by creating a Spark session. This is identical to batch Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0301178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StructuredStreamingLecture\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056444f2",
   "metadata": {},
   "source": [
    "### Defining a Schema\n",
    "In streaming, Spark cannot infer schema repeatedly. We must define it explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = '''\n",
    "pickup_datetime TIMESTAMP,\n",
    "passenger_count INT,\n",
    "fare_amount DOUBLE\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b869b67",
   "metadata": {},
   "source": [
    "### Reading Streaming Data\n",
    "This simulates streaming by watching a directory where new Parquet files arrive continuously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fed312",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = spark.readStream \\\n",
    "    .schema(schema) \\\n",
    "    .parquet('https://github.com/msfasha/307401-Big-Data/blob/main/20251/datasets/yellow_tripdata_2025-10.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4807fa",
   "metadata": {},
   "source": [
    "### Event Time, Watermarks, and Windows\n",
    "Real streaming data often arrives late. Spark uses **watermarks** to limit how long it waits for late data and to manage state size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, window\n",
    "\n",
    "agg_df = stream_df \\\n",
    "    .withWatermark('pickup_datetime', '10 minutes') \\\n",
    "    .groupBy(window(col('pickup_datetime'), '5 minutes')) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29691d1b",
   "metadata": {},
   "source": [
    "### Starting the Streaming Query\n",
    "Streaming computations only begin when we write the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734bfacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = agg_df.writeStream \\\n",
    "    .format('console') \\\n",
    "    .outputMode('append') \\\n",
    "    .option('checkpointLocation', '/tmp/checkpoints/ny_taxi') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9adb16a",
   "metadata": {},
   "source": [
    "## Part 2 – Why Spark Is Fast\n",
    "\n",
    "Spark performance is largely due to **how it reads data**, not just how fast it computes.\n",
    "\n",
    "We now examine column pruning, predicate pushdown, and partition pruning using Parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdcc48e",
   "metadata": {},
   "source": [
    "### Reading Parquet Data\n",
    "Parquet is a columnar format optimized for analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46dc891",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = spark.read.parquet('/data/ny_taxi_parquet/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4edaead",
   "metadata": {},
   "source": [
    "### Column Pruning and Predicate Pushdown\n",
    "Spark only reads the columns we need and pushes filters into the Parquet reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84901982",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = taxi_df \\\n",
    "    .select('pickup_datetime', 'fare_amount', 'passenger_count') \\\n",
    "    .filter((col('fare_amount') > 20) & (col('passenger_count') == 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d302542",
   "metadata": {},
   "source": [
    "### Examining the Physical Plan\n",
    "Understanding the physical plan is essential for debugging performance issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4db5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c64b41",
   "metadata": {},
   "source": [
    "### Partitioning Data\n",
    "Partitioning allows Spark to skip entire directories during reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ffda8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.write \\\n",
    "    .mode('overwrite') \\\n",
    "    .partitionBy('pickup_year', 'pickup_month') \\\n",
    "    .parquet('/data/ny_taxi_partitioned/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbc33db",
   "metadata": {},
   "source": [
    "### Partition Pruning in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet('/data/ny_taxi_partitioned/') \\\n",
    "    .filter(col('pickup_year') == 2022) \\\n",
    "    .explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c0029",
   "metadata": {},
   "source": [
    "## Part 3 – Delta Lake\n",
    "\n",
    "Parquet alone does not support transactions, updates, or deletes. Delta Lake adds these capabilities using a transaction log."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e70903",
   "metadata": {},
   "source": [
    "### Writing a Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d52a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.write \\\n",
    "    .format('delta') \\\n",
    "    .mode('overwrite') \\\n",
    "    .save('/data/delta/ny_taxi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2081fb",
   "metadata": {},
   "source": [
    "### Reading Delta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15abff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_df = spark.read \\\n",
    "    .format('delta') \\\n",
    "    .load('/data/delta/ny_taxi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb5aff7",
   "metadata": {},
   "source": [
    "### Time Travel\n",
    "Delta Lake allows querying previous versions of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727477b",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df = spark.read \\\n",
    "    .format('delta') \\\n",
    "    .option('versionAsOf', 0) \\\n",
    "    .load('/data/delta/ny_taxi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bce8b88",
   "metadata": {},
   "source": [
    "### MERGE (Upserts)\n",
    "Delta Lake supports SQL-style upserts using MERGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e158aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, '/data/delta/ny_taxi')\n",
    "updates_df = spark.read.parquet('/data/ny_taxi_updates/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0bd7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table.alias('t') \\\n",
    "    .merge(updates_df.alias('u'), 't.trip_id = u.trip_id') \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e04d2",
   "metadata": {},
   "source": [
    "### Streaming into Delta Lake\n",
    "This enables production-grade lakehouse architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df.writeStream \\\n",
    "    .format('delta') \\\n",
    "    .outputMode('append') \\\n",
    "    .option('checkpointLocation', '/tmp/checkpoints/delta') \\\n",
    "    .start('/data/delta/ny_taxi_streaming')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
