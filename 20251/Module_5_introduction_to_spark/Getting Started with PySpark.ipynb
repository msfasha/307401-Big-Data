{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/msfasha/307401-Big-Data/blob/main/20251/Module_5_introduction_to_spark/Introduction%20to%20Apache%20Spark%20Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfe8ca9-265b-44bc-81ed-2725eb5b93df",
   "metadata": {
    "id": "3dfe8ca9-265b-44bc-81ed-2725eb5b93df"
   },
   "source": [
    "## Getting Started with PySpark\n",
    "Setting Up PySpark\n",
    "You can run PySpark on a local machine or on a distributed cluster. For classroom purposes, we’ll focus on setting up PySpark locally with a Jupyter Notebook environment.\n",
    "### Installation Steps:\n",
    "1.\tInstall Java (required for Spark):<br>\n",
    "default-jre\n",
    "2.\tInstall PySpark:<br>\n",
    "pip install pyspark\n",
    "2. Running Your First PySpark Program<br>\n",
    "You can check if PySpark is installed correctly by launching a Jupyter Notebook and running this code to initialize Spark:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c495d6-cb77-400b-8f28-78b33b71ab9f",
   "metadata": {
    "id": "62c495d6-cb77-400b-8f28-78b33b71ab9f"
   },
   "source": [
    "### Creating a spark session\n",
    "SparkSession is the entry point to programming with Spark. It allows you to interact with Spark, load and process data, and manage resources in a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e9a9e4-737b-4bb0-9b82-cd4e1e2543e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6e9a9e4-737b-4bb0-9b82-cd4e1e2543e0",
    "outputId": "e45c4407-63d9-4f1c-ddbe-945f73d7ea44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Introduction to Spark\").getOrCreate()\n",
    "\n",
    "# Display Spark version\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d898bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The command above is equivalent to this command which explicitly uses all the availabe cores:\n",
    "# SparkSession.builder.master(\"local[*]\").appName(\"...\").getOrCreate()\n",
    "# SparkSession.builder.master(\"local[2]\").appName(\"Limited Spark\").getOrCreate() // limits to 2 cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5766f",
   "metadata": {},
   "source": [
    "Check the number of available cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11634384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.cpu_count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c4f659-ff5a-4705-92c4-d66c2441f0f2",
   "metadata": {
    "id": "f6c4f659-ff5a-4705-92c4-d66c2441f0f2"
   },
   "source": [
    "- SparkSession.builder: This starts the construction of a Spark session.\n",
    "- .appName(\"Introduction to Spark\"): This sets the name of your Spark application. It is used for identification in Spark's UI and logs.\n",
    "- .getOrCreate(): This either retrieves the existing Spark session (if one already exists) or creates a new one if none exists.\n",
    "This creates a Spark session object named spark, which you will use to interact with Spark.\n",
    "\n",
    "This initializes Spark and shows the version number, confirming that your environment is ready.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf578cf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27117735-62d0-45b4-8282-c5fa9aa356d8",
   "metadata": {
    "id": "27117735-62d0-45b4-8282-c5fa9aa356d8"
   },
   "source": [
    "### Resilient Distributed Datasets (RDDs)\n",
    "#### What is an RDD?\n",
    "RDDs, or Resilient Distributed Datasets, are the foundational data structure in Spark. They represent an immutable distributed collection of objects that can be processed in parallel across the nodes in a Spark cluster.\n",
    "Key properties of RDDs:\n",
    "- Resilient: RDDs automatically recover from node failures.\n",
    "- Distributed: Data is spread across multiple nodes.\n",
    "- Immutable: Once created, an RDD cannot be changed.\n",
    "#### Creating RDDs\n",
    "There are two main ways to create RDDs:\n",
    "- Parallelizing a collection: Creating an RDD from an existing list or array.\n",
    "- Reading from an external data source: Creating an RDD from a file or dataset (like a CSV file)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adb55d6-f9fc-48f3-b94e-0d32f1513c58",
   "metadata": {
    "id": "3adb55d6-f9fc-48f3-b94e-0d32f1513c58"
   },
   "source": [
    "#### Examples:\n",
    "1. Parallelize a Collection:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "805c4d5b-596b-449e-9109-2841bd3ebb5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "805c4d5b-596b-449e-9109-2841bd3ebb5c",
    "outputId": "dd8e6311-f820-431b-d430-c63afb74c79d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Collect the RDD data and print it\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f06c9c-4d57-42b0-8d17-1b838f2b1623",
   "metadata": {
    "id": "01f06c9c-4d57-42b0-8d17-1b838f2b1623"
   },
   "source": [
    "2.\tRead from a Text File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc8158cf-7bc7-4105-838f-0fa6abe2ed6a",
   "metadata": {
    "id": "bc8158cf-7bc7-4105-838f-0fa6abe2ed6a"
   },
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile(\"path/to/file.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc24ce29",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Tansformations and Actions\n",
    "In Apache Spark, **Transformations** and **Actions** are the two main types of operations used to process and analyze data. Understanding the difference between them is crucial for mastering Spark.\n",
    "\n",
    "Transformations are functions executed on demand to produce a new RDD. All transformations are followed by actions. Some examples of transformations include map, filter, and reduceByKey.\n",
    "\n",
    "Actions are the results of RDD computations or transformations. After an action is performed, the data from RDD moves back to the local machine. Some examples of actions include reduce, collect, first, and take.\n",
    "\n",
    "### 2.1. Transformations\n",
    "Transformations are **lazy operations** that define a set of instructions for manipulating data but do not execute them immediately. Instead, they create a new Resilient Distributed Dataset (RDD) or DataFrame, representing a logical plan for execution.\n",
    "\n",
    "#### Key Characteristics\n",
    "- **Lazy Evaluation**: Spark doesn’t execute transformations until an action triggers the computation.\n",
    "- **Immutable Data**: Transformations create new RDDs or DataFrames rather than modifying existing ones.\n",
    "- **Chaining**: Multiple transformations can be chained together to create complex workflows.\n",
    "\n",
    "#### Common Transformations\n",
    "| Transformation | Description                                  | Example |\n",
    "|-----------------|----------------------------------------------|---------|\n",
    "| `map()`         | Applies a function to each element in the dataset. | Transform each number to its square. |\n",
    "| `filter()`      | Filters elements based on a condition.      | Keep only even numbers. |\n",
    "| `flatMap()`     | Similar to `map()`, but can produce multiple output elements for each input element. | Split lines of text into words. |\n",
    "| `distinct()`    | Removes duplicate elements.                 | Get unique values in a dataset. |\n",
    "| `union()`       | Combines two datasets into one.             | Combine two RDDs. |\n",
    "| `groupByKey()`  | Groups data by key (key-value RDD).         | Group all values by their keys. |\n",
    "| `join()`        | Performs a join operation on two datasets.  | Join two RDDs/DataFrames. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af36da0",
   "metadata": {},
   "source": [
    "1. map(): Applies a function to each element in the RDD and returns a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1cce5e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Square each number\n",
    "squared_rdd = rdd.map(lambda x: x**2)\n",
    "\n",
    "print(squared_rdd.collect())  # Output: [1, 4, 9, 16, 25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf68916",
   "metadata": {},
   "source": [
    "**Parallelization**:\n",
    "   The `spark.sparkContext.parallelize(data)` function distributes the data across multiple cores or nodes in the cluster. Spark divides the dataset into partitions, and each partition can be processed independently.\n",
    "\n",
    "**Transformations and Actions**:\n",
    "   - The `filter` and `map` operations are **transformations**. They are lazy and only define the computation to be performed.\n",
    "   - The `collect` operation is an **action**. It triggers execution (map in this case) and aggregates the results back to the driver (local machine) and returns them as a Python list.\n",
    "\n",
    "**Scaling**:\n",
    "   - If you run this code in a Spark cluster with multiple cores or nodes, Spark will distribute the transformations (`filter` and `map`) across all available resources.\n",
    "   - Each core or executor will work on a subset of the data, speeding up the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23522d11",
   "metadata": {},
   "source": [
    "2. filter(): Returns a new RDD containing only the elements that satisfy a given condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2301e73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "even_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "print(even_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782fa1d7",
   "metadata": {},
   "source": [
    "3. flatMap(): Similar to map(), but flattens the results.\n",
    "map() transformation is applied to each row in a dataset to return a new dataset. flatMap() transformation is also used for each dataset row, but a new flattened dataset is returned. In the case of flatMap, if a record is nested (e.g., a column that is in itself made up of a list or array), the data within that record gets extracted and is returned as a new row of the returned dataset.\n",
    "\n",
    "Both map() and flatMap() transformations are narrow, meaning they do not result in the shuffling of data in Spark.\n",
    "- flatMap() is a one-to-many transformation function that returns more rows than the current DataFrame. Map() returns the same number of records as in the input DataFrame.\n",
    "- flatMap() can give a result that contains redundant data in some columns.\n",
    "- flatMap() can flatten a column that contains arrays or lists. It can be used to flatten any other nested collection too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cd1169ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'how', 'are', 'you']\n"
     ]
    }
   ],
   "source": [
    "lines = spark.sparkContext.parallelize([\"hello world\", \"how are you\"])\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "print(words.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a256e8",
   "metadata": {},
   "source": [
    "4. distinct(): Removes duplicate elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e7675c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "rdd_with_duplicates = spark.sparkContext.parallelize([1, 2, 2, 3, 4])\n",
    "distinct_rdd = rdd_with_duplicates.distinct()\n",
    "print(distinct_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d06b55",
   "metadata": {},
   "source": [
    "5. union(): Combines two RDDs into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ebb7336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = spark.sparkContext.parallelize([1, 2, 3])\n",
    "rdd2 = spark.sparkContext.parallelize([4, 5, 6])\n",
    "combined_rdd = rdd1.union(rdd2)\n",
    "print(combined_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb927f5",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "Write Python code to do the following:\n",
    "- Create an Array that has 10 million numbers.\n",
    "- Convert that array into an RDD using PySpark\n",
    "- Write code to compute the average of the numbers in the RDD using PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5311340b",
   "metadata": {},
   "source": [
    "### 2.2. Actions\n",
    "Actions are **eager operations** that trigger the execution of transformations. They perform computations and return a result to the driver program or write the output to an external storage.\n",
    "\n",
    "#### Key Characteristics\n",
    "- **Trigger Execution**: Actions force Spark to evaluate the transformations and perform the computation.\n",
    "- **Return Results**: They either return a value to the driver or save the result to a file system.\n",
    "- **Irreversible**: Actions mark the end of a computation chain.\n",
    "\n",
    "#### **Common Actions**\n",
    "| Action         | Description                                    | Example |\n",
    "|----------------|------------------------------------------------|---------|\n",
    "| `collect()`    | Returns all elements of the dataset as a list. | Collect results from an RDD. |\n",
    "| `count()`      | Counts the number of elements in the dataset.  | Find the total number of rows. |\n",
    "| `first()`      | Returns the first element of the dataset.      | Get the first line in a file. |\n",
    "| `take(n)`      | Returns the first `n` elements of the dataset. | Get the first 5 rows. |\n",
    "| `reduce()`     | Aggregates data using a specified function.    | Find the sum of all numbers. |\n",
    "| `saveAsTextFile()` | Saves the dataset to a text file.          | Save results to HDFS or local storage. |\n",
    "| `show()`       | Displays the first few rows of a DataFrame.    | Show data in tabular format. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a63408",
   "metadata": {},
   "source": [
    "Some common actions include:\n",
    "1.\tcollect(): Returns all elements of the RDD as a list (use sparingly with large datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8261ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected elements: [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "print(\"Collected elements:\", rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265346c1",
   "metadata": {},
   "source": [
    "2.\tcount(): Counts the number of elements in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4ee59e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of elements: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Count of elements:\", rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe205b3",
   "metadata": {},
   "source": [
    "3.\tfirst(): Returns the first element in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "604194cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"First element:\", rdd.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f73b558",
   "metadata": {},
   "source": [
    "4.\ttake(n): Returns the first n elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c596f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First three elements: [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"First three elements:\", rdd.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edc8915",
   "metadata": {},
   "source": [
    "5.\treduce(): Aggregates the elements of the RDD using a specified function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01a5b3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of elements: 15\n"
     ]
    }
   ],
   "source": [
    "# Sum all elements\n",
    "sum_of_elements = rdd.reduce(lambda x, y: x + y)\n",
    "print(\"Sum of elements:\", sum_of_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14cd82a",
   "metadata": {},
   "source": [
    "6.\tcountByValue(): Returns a dictionary of each unique value and its count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e2c9eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts: defaultdict(<class 'int'>, {1: 1, 2: 1, 3: 1, 4: 1, 5: 1})\n"
     ]
    }
   ],
   "source": [
    "value_counts = rdd.countByValue()\n",
    "print(\"Value counts:\", value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123be4d",
   "metadata": {},
   "source": [
    "## Transformations vs. Actions\n",
    "| Feature              | Transformations                        | Actions                              |\n",
    "|----------------------|----------------------------------------|-------------------------------------|\n",
    "| **Execution**         | Lazy: Build a logical execution plan. | Eager: Trigger computation.         |\n",
    "| **Output**            | Produces a new RDD/DataFrame.         | Returns a value or writes to storage. |\n",
    "| **Examples**          | `map()`, `filter()`, `flatMap()`      | `collect()`, `count()`, `show()`    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f3945e",
   "metadata": {},
   "source": [
    "### Why Lazy Evaluation Matters\n",
    "Lazy evaluation allows Spark to optimize the execution plan:\n",
    "1. **Minimizing Data Movement**: Spark analyzes the entire computation chain to reduce shuffling.\n",
    "2. **Combining Operations**: Spark can merge multiple transformations into a single stage.\n",
    "\n",
    "\n",
    "### Key Takeaway\n",
    "- Use **transformations** to define how data should be manipulated.\n",
    "- Use **actions** to trigger execution and extract results.\n",
    "- Understanding their roles helps you write efficient and optimized Spark applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b07793a-da41-4473-824a-6c8c982ed6a9",
   "metadata": {
    "id": "3b07793a-da41-4473-824a-6c8c982ed6a9"
   },
   "source": [
    "## Practical Example: Word Count with RDDs\n",
    "Problem: Count the occurrences of each word in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73ae8f1c-d4a0-4fd7-b7d5-c044a84a8f7d",
   "metadata": {
    "id": "73ae8f1c-d4a0-4fd7-b7d5-c044a84a8f7d",
    "outputId": "a8005d70-2643-412c-e1b0-b447a1f94103"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_78360/2155919677.py:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text_rdd = spark.sparkContext.textFile(\"datasets\\social_media_comments\\sentimentdataset.txt\")\n",
      "/tmp/ipykernel_78360/2155919677.py:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text_rdd = spark.sparkContext.textFile(\"datasets\\social_media_comments\\sentimentdataset.txt\")\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o426.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/media/me/Disk1-Repo 1/my_code/my_courses/307401-Big-Data/Apache Spark/datasets\\social_media_comments\\sentimentdataset.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.IOException: Input path does not exist: file:/media/me/Disk1-Repo 1/my_code/my_courses/307401-Big-Data/Apache Spark/datasets\\social_media_comments\\sentimentdataset.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 25 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m word_pairs \u001b[38;5;241m=\u001b[39m words_rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m word: (word, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Reduce by key (word) to count occurrences\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m word_counts \u001b[38;5;241m=\u001b[39m \u001b[43mword_pairs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduceByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Collect and display results\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, count \u001b[38;5;129;01min\u001b[39;00m word_counts\u001b[38;5;241m.\u001b[39mcollect():\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/pyspark/rdd.py:3552\u001b[0m, in \u001b[0;36mRDD.reduceByKey\u001b[0;34m(self, func, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   3505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduceByKey\u001b[39m(\n\u001b[1;32m   3506\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Tuple[K, V]]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3507\u001b[0m     func: Callable[[V, V], V],\n\u001b[1;32m   3508\u001b[0m     numPartitions: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3509\u001b[0m     partitionFunc: Callable[[K], \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m portable_hash,\n\u001b[1;32m   3510\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Tuple[K, V]]\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3511\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3512\u001b[0m \u001b[38;5;124;03m    Merge the values for each key using an associative and commutative reduce function.\u001b[39;00m\n\u001b[1;32m   3513\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3550\u001b[0m \u001b[38;5;124;03m    [('a', 2), ('b', 1)]\u001b[39;00m\n\u001b[1;32m   3551\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombineByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumPartitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitionFunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/pyspark/rdd.py:3975\u001b[0m, in \u001b[0;36mRDD.combineByKey\u001b[0;34m(self, createCombiner, mergeValue, mergeCombiners, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   3913\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3914\u001b[0m \u001b[38;5;124;03mGeneric function to combine the elements for each key using a custom\u001b[39;00m\n\u001b[1;32m   3915\u001b[0m \u001b[38;5;124;03mset of aggregation functions.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3972\u001b[0m \u001b[38;5;124;03m[('a', [1, 2]), ('b', [1])]\u001b[39;00m\n\u001b[1;32m   3973\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numPartitions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3975\u001b[0m     numPartitions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_defaultReducePartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3977\u001b[0m serializer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mserializer\n\u001b[1;32m   3978\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_limit()\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/pyspark/rdd.py:4867\u001b[0m, in \u001b[0;36mRDD._defaultReducePartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39mdefaultParallelism\n\u001b[1;32m   4866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/pyspark/rdd.py:5453\u001b[0m, in \u001b[0;36mPipelinedRDD.getNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetNumPartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m-> 5453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o426.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/media/me/Disk1-Repo 1/my_code/my_courses/307401-Big-Data/Apache Spark/datasets\\social_media_comments\\sentimentdataset.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: java.io.IOException: Input path does not exist: file:/media/me/Disk1-Repo 1/my_code/my_courses/307401-Big-Data/Apache Spark/datasets\\social_media_comments\\sentimentdataset.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 25 more\n"
     ]
    }
   ],
   "source": [
    "# Read the text file\n",
    "text_rdd = spark.sparkContext.textFile(\"datasets\\social_media_comments\\sentimentdataset.txt\")\n",
    "\n",
    "# Split each line into words and flatten\n",
    "words_rdd = text_rdd.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Map each word to a (word, 1) pair\n",
    "word_pairs = words_rdd.map(lambda word: (word, 1))\n",
    "\n",
    "# Reduce by key (word) to count occurrences\n",
    "word_counts = word_pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Collect and display results\n",
    "for word, count in word_counts.collect():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eddda2f-0665-4848-9d78-0d9e9efc3e81",
   "metadata": {
    "id": "1eddda2f-0665-4848-9d78-0d9e9efc3e81"
   },
   "source": [
    "### Code Explaination\n",
    "```python\n",
    "# Read the text file\n",
    "text_rdd = spark.sparkContext.textFile(\"datasets/social_media_comments/sentimentdataset.txt\")\n",
    "```\n",
    "Reads the file and divides it into chunks that are distributed across worker nodes. Each worker is responsible for processing its assigned lines, allowing for parallel reading.\n",
    "\n",
    "```python\n",
    "# Split each line into words and flatten\n",
    "words_rdd = text_rdd.flatMap(lambda line: line.split(\" \"))\n",
    "```\n",
    "Splits each line into words within each worker. Since the data was already distributed in the previous step, flatMap simply applies the splitting operation on each worker's assigned lines independently.\n",
    "\n",
    "```python\n",
    "# Map each word to a (word, 1) pair\n",
    "word_pairs = words_rdd.map(lambda word: (word, 1))\n",
    "```\n",
    "Converts each word to a `(word, 1)` pair on each worker. The `map` function sends this transformation to each worker, where it operates on its data independently.\n",
    "\n",
    "```python\n",
    "# Reduce by key (word) to count occurrences\n",
    "word_counts = word_pairs.reduceByKey(lambda x, y: x + y)\n",
    "```\n",
    "Aggregates the counts by word. Initially, each worker performs a local aggregation for the words it holds. Then, a shuffle occurs, redistributing data so all occurrences of the same word go to the same worker for final aggregation.\n",
    "\n",
    "```python\n",
    "# Collect and display results\n",
    "for word, count in word_counts.collect():\n",
    "    print(f\"{word}: {count}\")\n",
    "```\n",
    "Collects the final counts from all workers to the driver program. Each worker sends its results to the driver, where the data is combined and displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409e1b58",
   "metadata": {},
   "source": [
    "#### sorting the counts in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056b038-619a-42f7-9367-92c65157b446",
   "metadata": {
    "id": "0056b038-619a-42f7-9367-92c65157b446",
    "outputId": "da3d194f-6079-4e34-dae3-ce41a2311a00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 2321\n",
      "the: 808\n",
      "of: 623\n",
      "a: 621\n",
      "in: 259\n",
      "to: 133\n",
      "and: 111\n",
      "with: 107\n",
      "for: 99\n",
      "on: 91\n"
     ]
    }
   ],
   "source": [
    "# Read the text file\n",
    "text_rdd = spark.sparkContext.textFile(\"datasets/social_media_comments/sentimentdataset.txt\")\n",
    "\n",
    "# Split each line into words and flatten\n",
    "words_rdd = text_rdd.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Map each word to a (word, 1) pair\n",
    "word_pairs = words_rdd.map(lambda word: (word, 1))\n",
    "\n",
    "# Reduce by key (word) to count occurrences\n",
    "word_counts = word_pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Sort by count in descending order and take the top 10\n",
    "top_10_words = word_counts.sortBy(lambda x: x[1], ascending=False).take(10)\n",
    "\n",
    "# Display the top 10 results\n",
    "for word, count in top_10_words:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9f21b6-1a3b-4e78-819f-58dfae2b48ea",
   "metadata": {
    "id": "ef9f21b6-1a3b-4e78-819f-58dfae2b48ea"
   },
   "source": [
    "This word count example demonstrates several core RDD concepts, including transformations (flatMap, map, reduceByKey) and actions (collect).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7c0f4-7551-468c-87d1-696236eb44bc",
   "metadata": {
    "id": "c8f7c0f4-7551-468c-87d1-696236eb44bc"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b88aa3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58c88ec3",
   "metadata": {},
   "source": [
    "## 3. Introducing Spark SQL Tutorial: Using `apartment_prices.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db47007",
   "metadata": {},
   "source": [
    "Spark SQL is a powerful module in Apache Spark for processing structured data. It enables SQL-like querying of data and integrates seamlessly with Spark’s core APIs.\n",
    "\n",
    "Key capabilities:\n",
    "- Query structured data using SQL.\n",
    "- Work with various data formats (CSV, JSON, Parquet).\n",
    "- Combine SQL with Spark’s DataFrame API for powerful analytics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7f739a",
   "metadata": {},
   "source": [
    "\n",
    "### Setting Up Spark SQL\n",
    "\n",
    "#### Create a SparkSession\n",
    "The `SparkSession` is the entry point for working with Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22990267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"Apartment Prices Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e9d3b0",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "We'll load the provided `apartment_prices.csv` into a Spark DataFrame for analysis.\n",
    "\n",
    "#### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac0393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Square_Area: integer (nullable = true)\n",
      " |-- Num_Rooms: integer (nullable = true)\n",
      " |-- Age_of_Building: integer (nullable = true)\n",
      " |-- Floor_Level: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      "\n",
      "+-----------+---------+---------------+-----------+-----+-------+\n",
      "|Square_Area|Num_Rooms|Age_of_Building|Floor_Level| City|  Price|\n",
      "+-----------+---------+---------------+-----------+-----+-------+\n",
      "|        162|        1|             15|         12|Amman|74900.0|\n",
      "|        152|        5|              8|          8|Aqaba|79720.0|\n",
      "|         74|        3|              2|          8|Irbid|43200.0|\n",
      "|        166|        1|              3|         18|Irbid|69800.0|\n",
      "|        131|        3|             14|         15|Aqaba|63160.0|\n",
      "+-----------+---------+---------------+-----------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file into a DataFrame\n",
    "df = spark.read.csv(\"datasets/apartment_prices.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the schema and a few rows of the dataset\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14196fef",
   "metadata": {},
   "source": [
    "### Registering the DataFrame as a SQL Table\n",
    "To query the dataset using SQL, we register the DataFrame as a temporary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f762b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"apartments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9abff8",
   "metadata": {},
   "source": [
    "### SQL Operations on the Dataset\n",
    "\n",
    "#### a. Basic SELECT Query**\n",
    "Retrieve all apartments located in \"Amman.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee132f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------------+-----------+-----+--------+\n",
      "|Square_Area|Num_Rooms|Age_of_Building|Floor_Level| City|   Price|\n",
      "+-----------+---------+---------------+-----------+-----+--------+\n",
      "|        162|        1|             15|         12|Amman| 74900.0|\n",
      "|        134|        4|              4|          4|Amman| 80300.0|\n",
      "|        163|        4|             18|         10|Amman| 85350.0|\n",
      "|         97|        4|             19|         12|Amman| 56650.0|\n",
      "|        117|        1|              4|         19|Amman| 72650.0|\n",
      "|        108|        1|              1|          4|Amman| 56600.0|\n",
      "|         74|        1|              5|          8|Amman| 41300.0|\n",
      "|        110|        5|             11|         19|Amman| 82500.0|\n",
      "|        110|        5|              5|         19|Amman| 88500.0|\n",
      "|         80|        5|              6|          9|Amman| 64000.0|\n",
      "|        132|        2|              9|          3|Amman| 63400.0|\n",
      "|        191|        5|             12|         19|Amman|117950.0|\n",
      "|        149|        3|             14|         12|Amman| 80050.0|\n",
      "|        143|        1|             19|          4|Amman| 54350.0|\n",
      "|        163|        1|              7|          2|Amman| 73350.0|\n",
      "|        191|        3|              9|          4|Amman| 95950.0|\n",
      "|        193|        1|             14|         15|Amman| 92850.0|\n",
      "|         73|        1|              6|         19|Amman| 50850.0|\n",
      "|         99|        5|              9|         13|Amman| 73550.0|\n",
      "|        183|        5|              6|          3|Amman|104350.0|\n",
      "+-----------+---------+---------------+-----------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"SELECT * FROM apartments WHERE City = 'Amman'\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e29591",
   "metadata": {},
   "source": [
    "#### b. Aggregations\n",
    "Calculate the average price of apartments grouped by the number of bedrooms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf6a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|Num_Rooms|         avg_price|\n",
      "+---------+------------------+\n",
      "|        1| 56153.36363636364|\n",
      "|        3|61384.903846153844|\n",
      "|        5| 76516.07843137255|\n",
      "|        4|  66747.1264367816|\n",
      "|        2| 57040.51546391752|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"SELECT Num_Rooms, AVG(Price) AS avg_price FROM apartments GROUP BY Num_Rooms\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13041e33",
   "metadata": {},
   "source": [
    "#### c. Sorting Data\n",
    "List the top 5 most expensive apartments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e97af44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------------+-----------+-----+--------+\n",
      "|Square_Area|Num_Rooms|Age_of_Building|Floor_Level| City|   Price|\n",
      "+-----------+---------+---------------+-----------+-----+--------+\n",
      "|        199|        4|              2|         16|Amman|123550.0|\n",
      "|        183|        5|              4|         19|Amman|122350.0|\n",
      "|        191|        5|             12|         19|Amman|117950.0|\n",
      "|        187|        4|              7|         19|Amman|116150.0|\n",
      "|        160|        5|              1|         15|Amman|111000.0|\n",
      "+-----------+---------+---------------+-----------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"SELECT * FROM apartments ORDER BY price DESC LIMIT 5\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef1f699",
   "metadata": {},
   "source": [
    "#### d. Filtering and Conditions\n",
    "Find apartments with more than 3 bedrooms and priced below 200,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4a666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------------+-----------+-----+--------+\n",
      "|Square_Area|Num_Rooms|Age_of_Building|Floor_Level| City|   Price|\n",
      "+-----------+---------+---------------+-----------+-----+--------+\n",
      "|        152|        5|              8|          8|Aqaba| 79720.0|\n",
      "|         80|        4|             14|          7|Aqaba| 41800.0|\n",
      "|        181|        4|             16|         16|Aqaba| 85160.0|\n",
      "|        134|        4|              4|          4|Amman| 80300.0|\n",
      "|        147|        5|              5|          6|Aqaba| 78920.0|\n",
      "|        159|        4|             16|          9|Irbid| 60700.0|\n",
      "|        163|        4|             18|         10|Amman| 85350.0|\n",
      "|         61|        4|              7|         18|Aqaba| 52960.0|\n",
      "|         97|        4|             19|         12|Amman| 56650.0|\n",
      "|        189|        4|             16|         13|Aqaba| 85040.0|\n",
      "|         80|        5|             19|         13|Aqaba| 47800.0|\n",
      "|         81|        4|             18|         19|Aqaba| 50160.0|\n",
      "|        110|        5|             11|         19|Amman| 82500.0|\n",
      "|        167|        4|             10|         12|Irbid| 72100.0|\n",
      "|        114|        5|              2|         18|Irbid| 75200.0|\n",
      "|        123|        4|              3|          6|Aqaba| 67280.0|\n",
      "|        190|        5|             12|         18|Aqaba| 99400.0|\n",
      "|        110|        5|              5|         19|Amman| 88500.0|\n",
      "|         80|        5|              6|          9|Amman| 64000.0|\n",
      "|        191|        5|             12|         19|Amman|117950.0|\n",
      "+-----------+---------+---------------+-----------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM apartments \n",
    "    WHERE Num_Rooms > 3 AND Price < 200000\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0114eb",
   "metadata": {},
   "source": [
    "### Writing Query Results to a File\n",
    "Save the filtered data (apartments in \"Amman\") to a new CSV file.\n",
    "```python\n",
    "result = spark.sql(\"SELECT * FROM apartments WHERE location = 'Amman'\")\n",
    "result.write.csv(\"/mnt/data/amman_apartments.csv\", header=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343a7d2b",
   "metadata": {},
   "source": [
    "### Using Built-in SQL Functions\n",
    "\n",
    "#### a. String Manipulation\n",
    "Convert all location names to uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f396781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-------+\n",
      "|location_upper|Square_Area|  Price|\n",
      "+--------------+-----------+-------+\n",
      "|         AMMAN|        162|74900.0|\n",
      "|         AQABA|        152|79720.0|\n",
      "|         IRBID|         74|43200.0|\n",
      "|         IRBID|        166|69800.0|\n",
      "|         AQABA|        131|63160.0|\n",
      "|         AQABA|         80|41800.0|\n",
      "|         AQABA|        162|68320.0|\n",
      "|         AQABA|        181|85160.0|\n",
      "|         AMMAN|        134|80300.0|\n",
      "|         AQABA|        147|78920.0|\n",
      "|         IRBID|        176|51800.0|\n",
      "|         IRBID|        159|60700.0|\n",
      "|         AMMAN|        163|85350.0|\n",
      "|         IRBID|        190|74000.0|\n",
      "|         IRBID|        112|44600.0|\n",
      "|         AQABA|         61|52960.0|\n",
      "|         IRBID|        147|65100.0|\n",
      "|         AMMAN|         97|56650.0|\n",
      "|         AQABA|        189|85040.0|\n",
      "|         AQABA|         80|47800.0|\n",
      "+--------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"SELECT UPPER(City) AS location_upper, Square_Area, Price FROM apartments\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68d05e",
   "metadata": {},
   "source": [
    "#### b. Numeric Functions\n",
    "Calculate the price per square foot for each apartment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d40f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-------+------------------+\n",
      "| City|Square_Area|  Price|    price_per_sqft|\n",
      "+-----+-----------+-------+------------------+\n",
      "|Amman|        162|74900.0|462.34567901234567|\n",
      "|Aqaba|        152|79720.0| 524.4736842105264|\n",
      "|Irbid|         74|43200.0| 583.7837837837837|\n",
      "|Irbid|        166|69800.0|420.48192771084337|\n",
      "|Aqaba|        131|63160.0| 482.1374045801527|\n",
      "|Aqaba|         80|41800.0|             522.5|\n",
      "|Aqaba|        162|68320.0| 421.7283950617284|\n",
      "|Aqaba|        181|85160.0|470.49723756906076|\n",
      "|Amman|        134|80300.0| 599.2537313432836|\n",
      "|Aqaba|        147|78920.0| 536.8707482993198|\n",
      "|Irbid|        176|51800.0| 294.3181818181818|\n",
      "|Irbid|        159|60700.0|381.76100628930817|\n",
      "|Amman|        163|85350.0| 523.6196319018405|\n",
      "|Irbid|        190|74000.0| 389.4736842105263|\n",
      "|Irbid|        112|44600.0| 398.2142857142857|\n",
      "|Aqaba|         61|52960.0| 868.1967213114754|\n",
      "|Irbid|        147|65100.0|442.85714285714283|\n",
      "|Amman|         97|56650.0|  584.020618556701|\n",
      "|Aqaba|        189|85040.0| 449.9470899470899|\n",
      "|Aqaba|         80|47800.0|             597.5|\n",
      "+-----+-----------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"SELECT City, Square_Area, Price, (Price / Square_Area) AS price_per_sqft FROM apartments\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60e502f",
   "metadata": {},
   "source": [
    "#### c. Statistical Analysis\n",
    "Find the minimum, maximum, and average apartment prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d8d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+\n",
      "|min_price|max_price|avg_price|\n",
      "+---------+---------+---------+\n",
      "|  15900.0| 123550.0| 63410.94|\n",
      "+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        MIN(price) AS min_price, \n",
    "        MAX(price) AS max_price, \n",
    "        AVG(price) AS avg_price \n",
    "    FROM apartments\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced4827",
   "metadata": {},
   "source": [
    "### End-to-End Example\n",
    "1. Load the dataset.\n",
    "2. Filter apartments with at least 2 bedrooms and priced below 150,000.\n",
    "3. Group them by location and calculate the average price.\n",
    "4. Save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08caa2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter data\n",
    "filtered_data = spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM apartments \n",
    "    WHERE Num_Rooms >= 2 AND price < 150000\n",
    "\"\"\")\n",
    "\n",
    "# Step 2: Group and aggregate\n",
    "aggregated_data = spark.sql(\"\"\"\n",
    "    SELECT City, AVG(price) AS avg_price\n",
    "    FROM apartments\n",
    "    WHERE Num_Rooms >= 2 AND price < 150000\n",
    "    GROUP BY City\n",
    "\"\"\")\n",
    "\n",
    "# Step 3: Save results to a file\n",
    "# aggregated_data.write.csv(\"/mnt/data/filtered_apartments.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6ae04",
   "metadata": {},
   "source": [
    "# Machine Learning with Apache Spark: Predicting Apartment Prices\n",
    "\n",
    "In this notebook, we'll explore the basics of machine learning in Apache Spark using the MLlib library. Specifically, we'll build a regression model to predict apartment prices based on features like square area, number of rooms, age of the building, and floor level. \n",
    "\n",
    "### Step 1: Setting Up the Spark Environment\n",
    "\n",
    "First, we need to set up a `SparkSession`, which is the main entry point for using Spark's DataFrame and MLlib capabilities. The `SparkSession` allows us to create and manipulate DataFrames and to access Spark's machine learning library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b13808d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 21:02:14 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"Apartment Price Prediction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516fa92d",
   "metadata": {},
   "source": [
    "### Step 2: Loading the Dataset\n",
    "\n",
    "Next, we load the dataset containing apartment information and prices. Spark can read various file formats; here, we’re loading a CSV file with headers and inferring the data types for each column. Once loaded, we display the schema and some sample rows to understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc5bc4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Square_Area: integer (nullable = true)\n",
      " |-- Num_Rooms: integer (nullable = true)\n",
      " |-- Age_of_Building: integer (nullable = true)\n",
      " |-- Floor_Level: integer (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      "\n",
      "+-----------+---------+---------------+-----------+-----+-------+\n",
      "|Square_Area|Num_Rooms|Age_of_Building|Floor_Level| City|  Price|\n",
      "+-----------+---------+---------------+-----------+-----+-------+\n",
      "|        162|        1|             15|         12|Amman|74900.0|\n",
      "|        152|        5|              8|          8|Aqaba|79720.0|\n",
      "|         74|        3|              2|          8|Irbid|43200.0|\n",
      "|        166|        1|              3|         18|Irbid|69800.0|\n",
      "|        131|        3|             14|         15|Aqaba|63160.0|\n",
      "|         80|        4|             14|          7|Aqaba|41800.0|\n",
      "|        162|        2|             11|         11|Aqaba|68320.0|\n",
      "|        181|        4|             16|         16|Aqaba|85160.0|\n",
      "|        134|        4|              4|          4|Amman|80300.0|\n",
      "|        147|        5|              5|          6|Aqaba|78920.0|\n",
      "|        176|        2|             14|          3|Irbid|51800.0|\n",
      "|        159|        4|             16|          9|Irbid|60700.0|\n",
      "|        163|        4|             18|         10|Amman|85350.0|\n",
      "|        190|        2|              7|         14|Irbid|74000.0|\n",
      "|        112|        2|             10|         11|Irbid|44600.0|\n",
      "|         61|        4|              7|         18|Aqaba|52960.0|\n",
      "|        147|        2|              1|         12|Irbid|65100.0|\n",
      "|         97|        4|             19|         12|Amman|56650.0|\n",
      "|        189|        4|             16|         13|Aqaba|85040.0|\n",
      "|         80|        5|             19|         13|Aqaba|47800.0|\n",
      "+-----------+---------+---------------+-----------+-----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_path = \"datasets/apartment_prices.csv\"  # Adjust the path if needed\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the schema and data\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f9d42",
   "metadata": {},
   "source": [
    "### Step 3: Data Preprocessing – Handling Categorical Data\n",
    "\n",
    "In machine learning, we need to convert categorical data into numerical representations. Here, the `City` column is a categorical feature that we need to transform. We use `StringIndexer` to assign a numeric index to each unique city, and then we apply `OneHotEncoder` to convert these indices into a one-hot encoded vector. This helps the model process categorical data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9a86353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# Convert the 'City' column to a numeric index\n",
    "indexer = StringIndexer(inputCol=\"City\", outputCol=\"CityIndex\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "# Convert the numeric index to one-hot encoding\n",
    "encoder = OneHotEncoder(inputCol=\"CityIndex\", outputCol=\"CityVec\")\n",
    "df = encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc390cc3",
   "metadata": {},
   "source": [
    "### Step 4: Feature Engineering – Assembling Features\n",
    "\n",
    "Spark’s MLlib expects the features for each data point to be in a single vector column. We use the `VectorAssembler` to combine `Square_Area`, `Num_Rooms`, `Age_of_Building`, `Floor_Level`, and the one-hot encoded `CityVec` column into a single `features` column. We also rename the `Price` column to `label`, as MLlib expects the target variable to be named `label`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c4ca1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            features|  label|\n",
      "+--------------------+-------+\n",
      "|[162.0,1.0,15.0,1...|74900.0|\n",
      "|[152.0,5.0,8.0,8....|79720.0|\n",
      "|[74.0,3.0,2.0,8.0...|43200.0|\n",
      "|[166.0,1.0,3.0,18...|69800.0|\n",
      "|[131.0,3.0,14.0,1...|63160.0|\n",
      "|[80.0,4.0,14.0,7....|41800.0|\n",
      "|[162.0,2.0,11.0,1...|68320.0|\n",
      "|[181.0,4.0,16.0,1...|85160.0|\n",
      "|[134.0,4.0,4.0,4....|80300.0|\n",
      "|[147.0,5.0,5.0,6....|78920.0|\n",
      "|[176.0,2.0,14.0,3...|51800.0|\n",
      "|[159.0,4.0,16.0,9...|60700.0|\n",
      "|[163.0,4.0,18.0,1...|85350.0|\n",
      "|[190.0,2.0,7.0,14...|74000.0|\n",
      "|[112.0,2.0,10.0,1...|44600.0|\n",
      "|[61.0,4.0,7.0,18....|52960.0|\n",
      "|[147.0,2.0,1.0,12...|65100.0|\n",
      "|[97.0,4.0,19.0,12...|56650.0|\n",
      "|[189.0,4.0,16.0,1...|85040.0|\n",
      "|[80.0,5.0,19.0,13...|47800.0|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Assemble features into a single vector\n",
    "assembler = VectorAssembler(inputCols=[\"Square_Area\", \"Num_Rooms\", \"Age_of_Building\", \"Floor_Level\", \"CityVec\"], outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Select the final columns for modeling\n",
    "df = df.select(\"features\", df[\"Price\"].alias(\"label\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30cd99a",
   "metadata": {},
   "source": [
    "### Step 5: Splitting the Dataset\n",
    "\n",
    "To evaluate our model, we need to split the data into training and test sets. Typically, 80% of the data is used for training, and 20% is used for testing. This allows us to train the model on one portion of the data and then test its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa17c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9625b4d1",
   "metadata": {},
   "source": [
    "### Step 6: Building and Training a Linear Regression Model\n",
    "\n",
    "Now, we initialize and train a **Linear Regression** model. Linear regression is a supervised learning algorithm commonly used for predicting numerical values. Here, it will help us predict apartment prices based on the features provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5656190f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/15 21:02:37 WARN Instrumentation: [03494bb4] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/11/15 21:02:37 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [371.418044269336,4978.707796345987,-1014.4774362162049,1035.5111204683565,11925.43813852181,-8114.944798383919]\n",
      "Intercept: -1623.4506090574075\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Train the model on the training data\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Print model coefficients and intercept\n",
    "print(f\"Coefficients: {lr_model.coefficients}\")\n",
    "print(f\"Intercept: {lr_model.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19107323",
   "metadata": {},
   "source": [
    "The output will show the coefficients (weights) for each feature, indicating how each feature impacts the apartment price, as well as the intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3589b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------------------+\n",
      "|            features|  label|        prediction|\n",
      "+--------------------+-------+------------------+\n",
      "|[60.0,3.0,14.0,3....|22000.0|16386.659892134998|\n",
      "|[61.0,3.0,11.0,2....|24300.0| 18765.99912458459|\n",
      "|[61.0,5.0,15.0,18...|55450.0|61274.065836811176|\n",
      "|[63.0,4.0,14.0,12...|46350.0| 51839.60484240993|\n",
      "|[65.0,3.0,19.0,16...|35400.0|34747.952296873205|\n",
      "|[67.0,2.0,14.0,8....|28120.0| 27300.37880640007|\n",
      "|[68.0,1.0,13.0,14...|36600.0|41846.071351871564|\n",
      "|[71.0,2.0,4.0,3.0...|30300.0| 25638.32494491376|\n",
      "|[73.0,2.0,17.0,1....|15900.0|11121.932121705046|\n",
      "|[74.0,1.0,4.0,3.0...|30640.0| 29888.81607975969|\n",
      "|[74.0,1.0,5.0,8.0...|41300.0| 45977.33238440708|\n",
      "|[74.0,2.0,13.0,10...|40300.0|44911.242931960136|\n",
      "|[74.0,5.0,19.0,10...|49300.0|53760.501703700866|\n",
      "|[76.0,2.0,11.0,4....|37200.0| 41469.96717012108|\n",
      "|[78.0,2.0,9.0,2.0...|31080.0|30245.297751633643|\n",
      "|[80.0,5.0,6.0,9.0...|64000.0|  68141.7055196592|\n",
      "|[83.0,1.0,10.0,18...|50350.0| 54602.81880643364|\n",
      "|[89.0,1.0,8.0,6.0...|29700.0| 26393.76556195607|\n",
      "|[93.0,4.0,2.0,11....|70850.0|  74120.3642846161|\n",
      "|[94.0,3.0,10.0,5....|38200.0| 35143.80538309395|\n",
      "+--------------------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE): 2875.771955223112\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Show predictions\n",
    "predictions.select(\"features\", \"label\", \"prediction\").show()\n",
    "\n",
    "# Evaluate model using RMSE\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "\n",
    "# Initialize RegressionEvaluator with R2 metric\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# Evaluate model using R2\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b88a20",
   "metadata": {},
   "source": [
    "The predictions DataFrame shows the actual price (label) alongside the model’s predicted price (prediction). \n",
    "\n",
    "The RMSE (Root Mean Squared Error) provides a quantitative measure of the model’s accuracy on the test data, where lower values indicate better model performance.\n",
    "\n",
    "The R-squared (R²) value represents the proportion of the variance in the target variable (e.g., price) that is explained by the model. An R² value closer to 1 indicates a better model fit, while a value closer to 0 indicates a poor fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ce0f3a",
   "metadata": {},
   "source": [
    "\n",
    "### Summary\n",
    "\n",
    "In this notebook, we demonstrated how to use Spark MLlib to build a regression model for predicting apartment prices. The workflow included data preprocessing, feature engineering, model training, and evaluation. This example highlights Spark's ability to handle machine learning tasks on large datasets in a distributed environment, making it an excellent tool for scalable data processing and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1cbf5d-6cff-4b1a-a2be-aabcc87eaa5c",
   "metadata": {
    "id": "2a1cbf5d-6cff-4b1a-a2be-aabcc87eaa5c"
   },
   "source": [
    "## Installing pyspark on local windows machine:\n",
    "- install python 3.9\n",
    "- add python to the path\n",
    "you can create a virtual environment and add it to the path\n",
    "- install pyspark, pip install pyspark\n",
    "- install java 11\n",
    "- add JAVA_HOME = java folder to environment variables\n",
    "- Under System Variables, click New to add new variables:\n",
    "Variable name: PYSPARK_PYTHON\n",
    "Variable value: python\n",
    "- Repeat to add another variable:\n",
    "Variable name: PYSPARK_DRIVER_PYTHON\n",
    "Variable value: python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a63c93",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0da61",
   "metadata": {},
   "source": [
    "**code-based comparison** that demonstrates how **PySpark** outperforms **pandas** in processing larger datasets—even on a single machine like Google Colab or your local environment.\n",
    "\n",
    "\n",
    "\n",
    "## **Objective:**\n",
    "\n",
    "We’ll compare PySpark and pandas on:\n",
    "\n",
    "* Generating and processing a large dataset (e.g., 10 million rows).\n",
    "* Performing a **group-by aggregation**, which is CPU-intensive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b751d943",
   "metadata": {},
   "source": [
    "### Benchmark Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe99078",
   "metadata": {},
   "source": [
    "### Pandas Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be438ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Generate large dataset\n",
    "n = 10_000_000\n",
    "np.random.seed(42)\n",
    "df_pandas = pd.DataFrame({\n",
    "    'category': np.random.randint(0, 1000, size=n),\n",
    "    'value': np.random.rand(n)\n",
    "})\n",
    "\n",
    "# Time groupby operation\n",
    "start = time.time()\n",
    "result_pandas = df_pandas.groupby('category')['value'].mean()\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Pandas time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d34fd35",
   "metadata": {},
   "source": [
    "### PySpark Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a38931c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Pandas vs Spark Benchmark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create Spark DataFrame\n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "# Time groupby operation\n",
    "start = time.time()\n",
    "result_spark = df_spark.groupBy(\"category\").agg(avg(\"value\").alias(\"mean_value\"))\n",
    "result_spark.collect()  # Trigger computation\n",
    "end = time.time()\n",
    "\n",
    "print(f\"PySpark time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065a94fc",
   "metadata": {},
   "source": [
    "### Expected Output\n",
    "\n",
    "On a system with 2–4 cores:\n",
    "\n",
    "* **Pandas** will likely take **5–10 seconds**\n",
    "* **PySpark** will likely complete in **2–4 seconds**, thanks to:\n",
    "\n",
    "  * Efficient execution engine\n",
    "  * Multi-threading support even on a local machine\n",
    "  * Lazy evaluation and optimization\n",
    "\n",
    "> Note: For small data (<100k rows), pandas is often faster due to lower overhead. But with **10 million+ rows**, PySpark's engine shines.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Feature         | Pandas             | PySpark                  |\n",
    "| --------------- | ------------------ | ------------------------ |\n",
    "| Memory handling | In-memory only     | Distributed & spillable  |\n",
    "| Threading       | Mostly single-core | Multi-core               |\n",
    "| Lazy execution  | ❌                  | ✅                        |\n",
    "| Optimization    | Manual             | Built-in Catalyst engine |\n",
    "| Best for        | Small-to-mid data  | Big or growing data      |\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
